---
title: "midterm"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Importing Data
```{r cars include=FALSE}

df = read.csv("cars.csv", header = T, stringsAsFactors = TRUE)

df = subset(df, select=c("manufacturer_name", "model_name", "transmission", "color", "odometer_value", "year_produced", "engine_fuel", "engine_has_gas", "engine_type", "engine_capacity", "body_type", "has_warranty",
"state", "drivetrain", "price_usd", "is_exchangeable", "number_of_photos",  "up_counter"))
names(df)[names(df) == 'price_usd'] <- 'price'

```


## Renaming features
```{r include=FALSE}
names(df)[names(df) == 'price_usd'] <- 'price'
```


Checking for the the facors and numerical features   
```{r include=FALSE}
str(df)
```
Looks perfectly fine

## Check for nulls
```{r include=FALSE}

colSums(is.na(df))

```

Engine Capacity has 10 null values
```{r include=FALSE}
#Droping the rows with null values 
df <- df[complete.cases(df), ]
```

## Summary of Dataset
```{r include=FALSE}
library(corrplot)
library(tidyverse)

df %>% select_if(is.numeric)->cars_numerical
```

#The dataset is collected from various web resources in order to explore the used cars market and try to build a model that effectively predicts the price of the car based on its parameters (both numerical and categorical)

#The data is scraped in Belarus (western Europe) on the 2nd of December 2019, with `r dim(df)[1]` rows and `r dim(df)[2]` features. There are `r dim(cars_numerical)[2]` numerical features and `r (dim(df)[2])-(dim(cars_numerical)[2])` categorical features


## Descriptive Statistics
```{r}
summary(cars_numerical)
install.packages("fBasics")

library(fBasics)

basicStats(cars_numerical)




```


##Target valrible distribution
```{r}
library(ggplot2)
p <-  ggplot(df, aes(price)) 
#p + geom_boxplot()
p + geom_density()
#df %>%  ggplot( aes(x=manufacturer_name, y=price_usd)) +   geom_boxplot(outlier.colour="red", #outlier.shape=8,outlier.size=4)
#p + geom_point(x='price_usd',y='manufacturer_name')
```





```{r}
#library("corrplot")
#dfcorr <- cor(df)
#corrplot.mixed(dfcorr)


corrplot(cor(cars_numerical), method = 'number')


```

## Including Plots
## Normality tests

This section checks the normality of numerical variables based on the Q-Q plot, histogram, and normality tests. The most common method for normality test is called *Shapiro-Wilk's method*, however, this test only works when the observation is less than 5000 in __R__,and our dataset is more extensive than this value, so we will use the *Kolmogorov-Smirnov (K-S) normality test* instead.

```{r, fig.height=10}
library(gridExtra)
plot1 = ggplot(cars_numerical, aes(sample = price)) + stat_qq() + stat_qq_line() + labs(title = 'Q-Q plot of price')
plot2 = ggplot(cars_numerical, aes(x = price)) + geom_histogram(bins=30) + labs(title = 'Histogram of price')

```

As we can find in the quantile-quantile plot and the histogram,`price` is not normally distributed, if we want use price as the dependent variable for a linear regression, it is necessary to transform it to a normal distribution after that.

```{r}
plot3 = ggplot(cars_numerical, aes(sample = odometer_value)) + stat_qq() + stat_qq_line() + labs(title = 'Q-Q plot of odometer_value')
plot4 = ggplot(cars_numerical, aes(x = odometer_value)) + geom_histogram(bins=30) + labs(title = 'Histogram of odometer_value')

plot5 = ggplot(cars_numerical, aes(sample = year_produced)) + stat_qq() + stat_qq_line() + labs(title = 'Q-Q plot of year_produced')
plot6 = ggplot(cars_numerical, aes(x = year_produced)) + geom_histogram(bins=30) + labs(title = 'Histogram of year_produced')

grid.arrange(plot3, plot4, plot5, plot6, ncol=2, nrow=2)
```

```{r}
plot7 = ggplot(cars_numerical, aes(sample = engine_capacity)) + stat_qq() + stat_qq_line() + labs(title = 'Q-Q plot of engine_capacity')
plot8 = ggplot(cars_numerical, aes(x = engine_capacity)) + geom_histogram(bins=30) + labs(title = 'Histogram of engine_capacity')

plot9 = ggplot(cars_numerical, aes(sample = number_of_photos)) + stat_qq() + stat_qq_line() + labs(title = 'Q-Q plot of number_of_photos')
plot10 = ggplot(cars_numerical, aes(x = number_of_photos)) + geom_histogram(bins=30) + labs(title = 'Histogram of number_of_photos')

grid.arrange(plot7, plot8, plot9, plot10, ncol=2, nrow=2)
```

The Q-Q plots and histograms also show evidence of non-normality. The `odometer_value`, `engine_capacity` and `number_of_photos` are right-skewed, while `year_produced` is left-skewed.

Now we apply the *Kolmogorov-Smirnov normality test* into our data. The null hypothesis of this test is 'sample distribution is normal'.

```{r, warning=FALSE}
ks.test(df$price, 'pnorm', mean=mean(df$price), sd=sd(df$price))
ks.test(df$odometer_value, 'pnorm', mean=mean(df$odometer_value), sd=sd(df$odometer_value))
ks.test(df$year_produced, 'pnorm', mean=mean(df$year_produced), sd=sd(df$year_produced))
ks.test(df$engine_capacity, 'pnorm', mean=mean(df$engine_capacity), sd=sd(df$engine_capacity))
ks.test(df$number_of_photos, 'pnorm', mean=mean(df$number_of_photos), sd=sd(df$number_of_photos))
```

The p-value of all the numeric variables are < 2e-16 which is less than 0.05, therefore we conclude that the distributions of all our numeric variables are significantly different from normal distribution. They have the same results with Q-Q plots and histograms.


```{r pressure, echo=FALSE}

#boxplot(df['manufacturer_name'], df['price'])
#car_lm = lm(price ~ ., data = df2)


#library(s20x)
#pairs20x(df2)
```
```{r}
#library(ezids)
#loadPkg("corrplot")
#corrplot(df2)
```


### 2nd Phase( Skipped for now)
```{r}
#loadPkg("leaps")
#reg.best10 <- leaps::regsubsets(price_usd~. , data = df, nvmax = 10, nbest = 1, method = "exhaustive")  

# leaps::regsubsets() - Model selection by exhaustive (default) search, forward or backward stepwise, or sequential replacement
#The plot will show the Adjust R^2 when using the variables across the bottom
#plot(reg.best10, scale = "adjr2", main = "Adjusted R^2")
#plot(reg.best10, scale = "r2", main = "R^2")
# In the "leaps" package, we can use scale=c("bic","Cp","adjr2","r2")
#plot(reg.best10, scale = "bic", main = "BIC")
#plot(reg.best10, scale = "Cp", main = "Cp")
#summary(reg.best10)
```


We can observe that the target variable is nor normally distributed which is an assumption of the linear regression model
Let try the box-Cox transformation to normalize the target variable

```{r}
library(MASS)
y = df$price
df %>% ggplot(aes(price)) + geom_density()

result = boxcox(y~1)
#Getting the optimal lambda value
mylambda = result$x[which.max(result$y)]
mylambda
#Transforming the price to new feature using the lambda value
# To get the original price, perform inverse operation to the one below
price_normal = (y^mylambda-1)/mylambda
data_frame(val=price_normal) %>% ggplot(aes(val)) + geom_density()

df <- cbind(df,price_normal)
```

